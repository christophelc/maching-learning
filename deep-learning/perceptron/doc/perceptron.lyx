#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Perceptron Memento
\end_layout

\begin_layout Author
Christophe Le cam
\end_layout

\begin_layout Section
Reference
\end_layout

\begin_layout Standard
Machine Learnia (Youtube)
\end_layout

\begin_layout Subsection
Perceptron
\end_layout

\begin_layout Subsubsection
Description
\end_layout

\begin_layout Standard
Input: 
\end_layout

\begin_layout Standard
\begin_inset Formula $x_{1},x_{2}$
\end_inset


\begin_inset Newline newline
\end_inset

Output: 
\end_layout

\begin_layout Standard
\begin_inset Formula $y_{pred}$
\end_inset


\begin_inset Newline newline
\end_inset

Aggregation function:
\end_layout

\begin_layout Standard
\begin_inset Formula $z(x_{1,}x_{2})=w_{1}x_{1}+w_{2}x_{2}+b$
\end_inset


\begin_inset Newline newline
\end_inset

Result: 
\end_layout

\begin_layout Standard
1 if 
\begin_inset Formula $z$
\end_inset

< 0, 0 
\end_layout

\begin_layout Standard
0 if z 
\begin_inset Formula $\geq0$
\end_inset


\end_layout

\begin_layout Subsubsection
Decision boundary:
\end_layout

\begin_layout Standard
\begin_inset Formula $z(x_{1,}x_{2})=w_{1}x_{1}+w_{2}x_{2}+b$
\end_inset

 = 0
\end_layout

\begin_layout Subsubsection
Probability
\end_layout

\begin_layout Standard
It measures how far a point is from the decision boundary.
 To indroduce it, we use the sigmoid function (logistic function).
\end_layout

\begin_layout Standard
\begin_inset Formula $a(z)=\frac{1}{1+e^{-x}}$
\end_inset


\begin_inset Newline newline
\end_inset

It transforms z to a a number a(z) between 0 and 1 that can be assimilated
 to a probability.
 Thus the stochastic variable Y follows the law of Bernouilli:
\end_layout

\begin_layout Standard
\begin_inset Formula $\ensuremath{\mathbb{P}}(Y=1)=a(z)^{y}+(1-a(z))^{1-y}$
\end_inset


\end_layout

\begin_layout Subsection
Learning
\end_layout

\begin_layout Subsubsection
Loss function
\end_layout

\begin_layout Standard
Let be m the number of observation.
 The Log loss is given by:
\end_layout

\begin_layout Standard
\begin_inset Formula $L=-\frac{1}{m}\sum_{i=1}^{m}y_{i}log(a_{i})+(1-y_{i})log(1-a_{i})$
\end_inset


\end_layout

\begin_layout Subsubsection
Gradien descent
\end_layout

\begin_layout Standard
We want to minimize the loss function by adjusting (W, b) parameters:
\end_layout

\begin_layout Standard
\begin_inset Formula $\nabla_{W}L$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $W_{t+1}=W_{t}-\alpha\nabla_{W_{t}}L$
\end_inset


\end_layout

\begin_layout Subsubsection
Computation of the gradient
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
a_{i} & =\frac{1}{1+e^{-z_{i}}}\\
1-a_{i} & =e^{-z}a_{i}\\
log(1-a_{i}) & =-z+loga_{i}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Thus:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
L & =-\frac{1}{m}\sum_{i=1}^{m}y_{i}loga_{i}+(1-y_{i})log(1-a_{i})\\
 & =-\frac{1}{m}\sum_{i=1}^{m}y_{i}loga_{i}+(1-y_{i})(-z+loga_{i})\\
 & =-\frac{1}{m}\sum_{i=1}^{m}(-z+zy_{i}-log(1+e^{-z})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Since:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial log(1-e^{-z})}{\partial z} & =-\frac{e^{-z}}{1+e^{-z}}\\
 & =e^{-z}a_{i}\\
 & =1-a_{i}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We have:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial L}{\partial w_{1}} & =\frac{\partial L}{\partial z}\frac{\partial z}{\partial w_{1}}\\
 & =-\frac{1}{m}\sum_{i=1}^{m}(-1+y_{i}+1-a_{i})x_{1}\\
 & =-\frac{1}{m}\sum_{i=1}^{m}(y_{i}-a_{i})x_{1}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Similarly:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial L}{\partial w_{2}} & =-\frac{1}{m}\sum_{i=1}^{m}(y_{i}-a_{i})x_{1}\\
\frac{\partial L}{\partial b} & =-\frac{1}{m}\sum_{i=1}^{m}(y_{i}-a_{i})
\end{align*}

\end_inset


\end_layout

\end_body
\end_document
